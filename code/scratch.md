# 随手笔记

kernel function f 以及 损失函数 F， 通过理论求出f对F的变分，来对f做梯度下降。

如果弱一步的话，可以预设f取某些结构，但这样理论上就不好玩了。

这是我的确定性算法能work的一个原因，就是kernel不局限于$sigma(Wx)$

同时不局限于一阶残差，这个残差超深的。

work的原因是

//还有个问题是函数一个个残差怎么加啊，如果累积的话求导算例一会儿就承受不起了，可能要累加几次然后跑一次压缩。

下面的工作还要对比它和传统CNN，是不是就是kernel的差别？可能是的。

在这个问题中

然后问题是：把f解剖为一个几层的神经网络？做出来？这个神经网络的复杂度与卷积核的大小成正相关。

kernel 是这样的一种形式：用relu+bias,全连接，从n的input size开始，每层的大小是上一层大小/2上取整，这样看起来比较美观，能学更多函数，看起来也能学卷积，并且对于n=5的这种情况min(a,b+1,c+1,d+1,e+1)是能直接100%学到的，不过它的复杂度是O(n^2)

对n=5, 是10 -(对位连) 5-5-3-3-2-2-1的全连接

mixup的操作是10000*(1- 0/1) + a,这样为0时就会被算出无穷大，从而被filter掉。

5-3 为 a,min（b,c),min(d,e)

3-2 为 a,min(x,y)

2-1 为 min(a,z+1)

注意到min是需要两层的，因此每一个全连接层会重复两遍。

因为

max(a,b) = relu(b-a) + b

min(a,b) = -relu(a-b)+b

应当使用pytorch的自动求导。

最后一层是any(a1,...,a100) = sum(relu(a1),...,relu(a100)) > 0,激活后一个聚合就ok了

这很像深度森林模型。

这不会就是GCN吧？！ 因为推广到图上不是也可以做完全一样的事情吗，而且那样每个节点维护的是一个向量。

oh，这似乎没什么关系，我甚至可以把这个方法apply在图上、但是一定要防重复，因此要读点GNN的论文。

二重实验对比是做围棋吃子判断。
